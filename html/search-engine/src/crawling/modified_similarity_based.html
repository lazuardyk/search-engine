<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>search-engine.src.crawling.modified_similarity_based API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>search-engine.src.crawling.modified_similarity_based</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from matplotlib.pyplot import hot
from src.database.database import Database
from src.crawling.page_content import PageContent
from src.crawling.util import Util
from datetime import datetime
from urllib.parse import urljoin
import bs4
import threading
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures.thread
import queue
import time
import re


class ModifiedSimilarityBased:
    &#34;&#34;&#34;Kelas yang digunakan untuk melakukan crawling dengan metode Modified Similarity Based Crawling.&#34;&#34;&#34;

    def __init__(self, crawl_id, url_queue, visited_urls, list_urls, keyword, duration_sec, max_threads):
        self.crawl_id = crawl_id
        self.visited_urls = visited_urls
        self.keyword = keyword
        self.duration_sec = duration_sec
        self.max_threads = max_threads
        self.db = Database()
        self.page_content = PageContent()
        self.util = Util()
        self.lock = threading.Lock()
        self.start_time = time.time()
        self.list_urls = list_urls
        self.hot_queue = queue.Queue()
        self.url_queue = self.reorder_queue(url_queue)

    def run(self):
        &#34;&#34;&#34;Fungsi utama yang berfungsi untuk menjalankan proses crawling MSB.&#34;&#34;&#34;
        executor = ThreadPoolExecutor(max_workers=self.max_threads)

        futures = []
        while True:
            try:
                time_now = time.time() - self.start_time
                time_now_int = int(time_now)
                if time_now_int &gt;= self.duration_sec:
                    print(&#34;Stopped because exceeded time limit...&#34;)
                    break
                if self.hot_queue.qsize() &gt; 0:
                    target_url = self.hot_queue.get()
                else:
                    target_url = self.url_queue.get()
                if target_url not in self.visited_urls:
                    self.visited_urls.append(target_url)
                    futures.append(executor.submit(self.scrape_page, target_url))

                self.hot_queue = self.reorder_queue(self.hot_queue)
                self.url_queue = self.reorder_queue(self.url_queue)
            except queue.Empty:
                if self.util.running_thread_count(futures) &gt; 0:
                    continue
                else:
                    print(&#34;Stopped because empty queue...&#34;)
                    break
            except KeyboardInterrupt:
                print(&#34;Stopped because keyboard interrupt...&#34;)
                break
            except Exception as e:
                print(e)
                continue

        executor._threads.clear()
        concurrent.futures.thread._threads_queues.clear()

    def scrape_page(self, url):
        &#34;&#34;&#34;Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.&#34;&#34;&#34;
        try:
            response = self.util.get_page(url)
            if response and response.status_code == 200:
                db_connection = self.db.connect()
                self.lock.acquire()
                now = datetime.now()
                print(url, &#34;| MSB |&#34;, now.strftime(&#34;%d/%m/%Y %H:%M:%S&#34;))
                self.lock.release()
                soup = bs4.BeautifulSoup(response.text, &#34;html.parser&#34;)
                title = soup.title.string
                article_html5 = soup.find(&#34;article&#34;)
                if article_html5 is None:
                    # extract text content from html4
                    html5 = 0
                    texts = soup.find(&#34;body&#34;).findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text
                else:
                    # extract text content from html5
                    html5 = 1
                    texts = article_html5.findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text

                # get meta description
                description = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;description&#34;})
                if description is None:
                    description = &#34;-&#34;
                else:
                    description = description.get(&#34;content&#34;)

                # get meta keywords
                keywords = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;keywords&#34;})
                if keywords is None:
                    keywords = &#34;-&#34;
                else:
                    keywords = keywords.get(&#34;content&#34;)

                # check hot_url
                hot_link = 0
                if (self.util.count_keyword_in_text(complete_text, self.keyword) &gt;= 10) or (
                    self.util.count_keyword_in_text(title, self.keyword) &gt;= 1
                ):
                    hot_link = 1

                # check if the page information already exist
                if not self.db.check_value_in_table(db_connection, &#34;page_information&#34;, &#34;url&#34;, url):
                    self.page_content.insert_page_information(
                        db_connection,
                        url,
                        self.crawl_id,
                        html5,
                        title,
                        description,
                        keywords,
                        complete_text,
                        hot_link,
                        &#34;MSB crawling&#34;,
                    )
                else:
                    self.db.close_connection(db_connection)
                    return

                # extract style
                for style in soup.findAll(&#34;style&#34;):
                    self.page_content.insert_page_style(db_connection, url, style)

                # extract script
                for script in soup.findAll(&#34;script&#34;):
                    self.page_content.insert_page_script(db_connection, url, script)

                # extract lists
                for lists in soup.findAll(&#34;li&#34;):
                    self.page_content.insert_page_list(db_connection, url, lists)

                # extract forms
                for form in soup.findAll(&#34;form&#34;):
                    self.page_content.insert_page_form(db_connection, url, form)

                # extract tables
                for table in soup.findAll(&#34;table&#34;):
                    self.page_content.insert_page_table(db_connection, url, table)

                # extract images
                for image in soup.findAll(&#34;img&#34;):
                    self.page_content.insert_page_image(db_connection, url, image)

                # extract outgoing link
                links = soup.findAll(&#34;a&#34;, href=True)
                for i in links:
                    # Complete relative URLs and strip trailing slash
                    complete_url = urljoin(url, i[&#34;href&#34;]).rstrip(&#34;/&#34;)

                    self.list_urls.append(complete_url)
                    self.page_content.insert_page_linking(db_connection, self.crawl_id, url, complete_url)

                    self.lock.acquire()
                    if self.util.is_valid_url(complete_url) and complete_url not in self.visited_urls:
                        if hot_link == 1 or self.keyword in url:
                            self.hot_queue.put(complete_url)
                        else:
                            self.url_queue.put(complete_url)
                    self.lock.release()

                self.db.close_connection(db_connection)
                return
            return
        except Exception as e:
            print(e, &#34;~ Error in thread&#34;)
            return

    def tag_visible(self, element):
        &#34;&#34;&#34;Fungsi untuk merapihkan konten teks.&#34;&#34;&#34;
        if element.parent.name in [&#34;style&#34;, &#34;script&#34;, &#34;head&#34;, &#34;title&#34;, &#34;meta&#34;, &#34;[document]&#34;]:
            return False
        if isinstance(element, bs4.element.Comment):
            return False
        if re.match(r&#34;[\n]+&#34;, str(element)):
            return False
        return True

    def reorder_queue(self, q):
        &#34;&#34;&#34;Fungsi untuk melakukan mengurutkan ulang antrian link pada crawler&#34;&#34;&#34;
        value_backlink = []
        for u in list(q.queue):
            # menentukan nilai backlink_count
            backlink_count = self.list_urls.count(u)
            # print(backlink_count)
            # memasukan backlink_count ke array value_backlink
            value_backlink.append(backlink_count)

        # membuat dictionary backlink untuk proses sorting
        backlink_dictionary = dict(zip(q.queue, value_backlink))

        # sorting backlink_dictionary
        sort_orders = sorted(backlink_dictionary.items(), key=lambda x: x[1], reverse=True)
        # mengkosongkan queue
        with q.mutex:
            q.queue.clear()

        # membuat queue yang sudah di sort
        for i in sort_orders:
            q.put(i[0])

        return q</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased"><code class="flex name class">
<span>class <span class="ident">ModifiedSimilarityBased</span></span>
<span>(</span><span>crawl_id, url_queue, visited_urls, list_urls, keyword, duration_sec, max_threads)</span>
</code></dt>
<dd>
<div class="desc"><p>Kelas yang digunakan untuk melakukan crawling dengan metode Modified Similarity Based Crawling.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModifiedSimilarityBased:
    &#34;&#34;&#34;Kelas yang digunakan untuk melakukan crawling dengan metode Modified Similarity Based Crawling.&#34;&#34;&#34;

    def __init__(self, crawl_id, url_queue, visited_urls, list_urls, keyword, duration_sec, max_threads):
        self.crawl_id = crawl_id
        self.visited_urls = visited_urls
        self.keyword = keyword
        self.duration_sec = duration_sec
        self.max_threads = max_threads
        self.db = Database()
        self.page_content = PageContent()
        self.util = Util()
        self.lock = threading.Lock()
        self.start_time = time.time()
        self.list_urls = list_urls
        self.hot_queue = queue.Queue()
        self.url_queue = self.reorder_queue(url_queue)

    def run(self):
        &#34;&#34;&#34;Fungsi utama yang berfungsi untuk menjalankan proses crawling MSB.&#34;&#34;&#34;
        executor = ThreadPoolExecutor(max_workers=self.max_threads)

        futures = []
        while True:
            try:
                time_now = time.time() - self.start_time
                time_now_int = int(time_now)
                if time_now_int &gt;= self.duration_sec:
                    print(&#34;Stopped because exceeded time limit...&#34;)
                    break
                if self.hot_queue.qsize() &gt; 0:
                    target_url = self.hot_queue.get()
                else:
                    target_url = self.url_queue.get()
                if target_url not in self.visited_urls:
                    self.visited_urls.append(target_url)
                    futures.append(executor.submit(self.scrape_page, target_url))

                self.hot_queue = self.reorder_queue(self.hot_queue)
                self.url_queue = self.reorder_queue(self.url_queue)
            except queue.Empty:
                if self.util.running_thread_count(futures) &gt; 0:
                    continue
                else:
                    print(&#34;Stopped because empty queue...&#34;)
                    break
            except KeyboardInterrupt:
                print(&#34;Stopped because keyboard interrupt...&#34;)
                break
            except Exception as e:
                print(e)
                continue

        executor._threads.clear()
        concurrent.futures.thread._threads_queues.clear()

    def scrape_page(self, url):
        &#34;&#34;&#34;Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.&#34;&#34;&#34;
        try:
            response = self.util.get_page(url)
            if response and response.status_code == 200:
                db_connection = self.db.connect()
                self.lock.acquire()
                now = datetime.now()
                print(url, &#34;| MSB |&#34;, now.strftime(&#34;%d/%m/%Y %H:%M:%S&#34;))
                self.lock.release()
                soup = bs4.BeautifulSoup(response.text, &#34;html.parser&#34;)
                title = soup.title.string
                article_html5 = soup.find(&#34;article&#34;)
                if article_html5 is None:
                    # extract text content from html4
                    html5 = 0
                    texts = soup.find(&#34;body&#34;).findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text
                else:
                    # extract text content from html5
                    html5 = 1
                    texts = article_html5.findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text

                # get meta description
                description = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;description&#34;})
                if description is None:
                    description = &#34;-&#34;
                else:
                    description = description.get(&#34;content&#34;)

                # get meta keywords
                keywords = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;keywords&#34;})
                if keywords is None:
                    keywords = &#34;-&#34;
                else:
                    keywords = keywords.get(&#34;content&#34;)

                # check hot_url
                hot_link = 0
                if (self.util.count_keyword_in_text(complete_text, self.keyword) &gt;= 10) or (
                    self.util.count_keyword_in_text(title, self.keyword) &gt;= 1
                ):
                    hot_link = 1

                # check if the page information already exist
                if not self.db.check_value_in_table(db_connection, &#34;page_information&#34;, &#34;url&#34;, url):
                    self.page_content.insert_page_information(
                        db_connection,
                        url,
                        self.crawl_id,
                        html5,
                        title,
                        description,
                        keywords,
                        complete_text,
                        hot_link,
                        &#34;MSB crawling&#34;,
                    )
                else:
                    self.db.close_connection(db_connection)
                    return

                # extract style
                for style in soup.findAll(&#34;style&#34;):
                    self.page_content.insert_page_style(db_connection, url, style)

                # extract script
                for script in soup.findAll(&#34;script&#34;):
                    self.page_content.insert_page_script(db_connection, url, script)

                # extract lists
                for lists in soup.findAll(&#34;li&#34;):
                    self.page_content.insert_page_list(db_connection, url, lists)

                # extract forms
                for form in soup.findAll(&#34;form&#34;):
                    self.page_content.insert_page_form(db_connection, url, form)

                # extract tables
                for table in soup.findAll(&#34;table&#34;):
                    self.page_content.insert_page_table(db_connection, url, table)

                # extract images
                for image in soup.findAll(&#34;img&#34;):
                    self.page_content.insert_page_image(db_connection, url, image)

                # extract outgoing link
                links = soup.findAll(&#34;a&#34;, href=True)
                for i in links:
                    # Complete relative URLs and strip trailing slash
                    complete_url = urljoin(url, i[&#34;href&#34;]).rstrip(&#34;/&#34;)

                    self.list_urls.append(complete_url)
                    self.page_content.insert_page_linking(db_connection, self.crawl_id, url, complete_url)

                    self.lock.acquire()
                    if self.util.is_valid_url(complete_url) and complete_url not in self.visited_urls:
                        if hot_link == 1 or self.keyword in url:
                            self.hot_queue.put(complete_url)
                        else:
                            self.url_queue.put(complete_url)
                    self.lock.release()

                self.db.close_connection(db_connection)
                return
            return
        except Exception as e:
            print(e, &#34;~ Error in thread&#34;)
            return

    def tag_visible(self, element):
        &#34;&#34;&#34;Fungsi untuk merapihkan konten teks.&#34;&#34;&#34;
        if element.parent.name in [&#34;style&#34;, &#34;script&#34;, &#34;head&#34;, &#34;title&#34;, &#34;meta&#34;, &#34;[document]&#34;]:
            return False
        if isinstance(element, bs4.element.Comment):
            return False
        if re.match(r&#34;[\n]+&#34;, str(element)):
            return False
        return True

    def reorder_queue(self, q):
        &#34;&#34;&#34;Fungsi untuk melakukan mengurutkan ulang antrian link pada crawler&#34;&#34;&#34;
        value_backlink = []
        for u in list(q.queue):
            # menentukan nilai backlink_count
            backlink_count = self.list_urls.count(u)
            # print(backlink_count)
            # memasukan backlink_count ke array value_backlink
            value_backlink.append(backlink_count)

        # membuat dictionary backlink untuk proses sorting
        backlink_dictionary = dict(zip(q.queue, value_backlink))

        # sorting backlink_dictionary
        sort_orders = sorted(backlink_dictionary.items(), key=lambda x: x[1], reverse=True)
        # mengkosongkan queue
        with q.mutex:
            q.queue.clear()

        # membuat queue yang sudah di sort
        for i in sort_orders:
            q.put(i[0])

        return q</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.reorder_queue"><code class="name flex">
<span>def <span class="ident">reorder_queue</span></span>(<span>self, q)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi untuk melakukan mengurutkan ulang antrian link pada crawler</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reorder_queue(self, q):
    &#34;&#34;&#34;Fungsi untuk melakukan mengurutkan ulang antrian link pada crawler&#34;&#34;&#34;
    value_backlink = []
    for u in list(q.queue):
        # menentukan nilai backlink_count
        backlink_count = self.list_urls.count(u)
        # print(backlink_count)
        # memasukan backlink_count ke array value_backlink
        value_backlink.append(backlink_count)

    # membuat dictionary backlink untuk proses sorting
    backlink_dictionary = dict(zip(q.queue, value_backlink))

    # sorting backlink_dictionary
    sort_orders = sorted(backlink_dictionary.items(), key=lambda x: x[1], reverse=True)
    # mengkosongkan queue
    with q.mutex:
        q.queue.clear()

    # membuat queue yang sudah di sort
    for i in sort_orders:
        q.put(i[0])

    return q</code></pre>
</details>
</dd>
<dt id="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi utama yang berfungsi untuk menjalankan proses crawling MSB.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;Fungsi utama yang berfungsi untuk menjalankan proses crawling MSB.&#34;&#34;&#34;
    executor = ThreadPoolExecutor(max_workers=self.max_threads)

    futures = []
    while True:
        try:
            time_now = time.time() - self.start_time
            time_now_int = int(time_now)
            if time_now_int &gt;= self.duration_sec:
                print(&#34;Stopped because exceeded time limit...&#34;)
                break
            if self.hot_queue.qsize() &gt; 0:
                target_url = self.hot_queue.get()
            else:
                target_url = self.url_queue.get()
            if target_url not in self.visited_urls:
                self.visited_urls.append(target_url)
                futures.append(executor.submit(self.scrape_page, target_url))

            self.hot_queue = self.reorder_queue(self.hot_queue)
            self.url_queue = self.reorder_queue(self.url_queue)
        except queue.Empty:
            if self.util.running_thread_count(futures) &gt; 0:
                continue
            else:
                print(&#34;Stopped because empty queue...&#34;)
                break
        except KeyboardInterrupt:
            print(&#34;Stopped because keyboard interrupt...&#34;)
            break
        except Exception as e:
            print(e)
            continue

    executor._threads.clear()
    concurrent.futures.thread._threads_queues.clear()</code></pre>
</details>
</dd>
<dt id="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.scrape_page"><code class="name flex">
<span>def <span class="ident">scrape_page</span></span>(<span>self, url)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_page(self, url):
    &#34;&#34;&#34;Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.&#34;&#34;&#34;
    try:
        response = self.util.get_page(url)
        if response and response.status_code == 200:
            db_connection = self.db.connect()
            self.lock.acquire()
            now = datetime.now()
            print(url, &#34;| MSB |&#34;, now.strftime(&#34;%d/%m/%Y %H:%M:%S&#34;))
            self.lock.release()
            soup = bs4.BeautifulSoup(response.text, &#34;html.parser&#34;)
            title = soup.title.string
            article_html5 = soup.find(&#34;article&#34;)
            if article_html5 is None:
                # extract text content from html4
                html5 = 0
                texts = soup.find(&#34;body&#34;).findAll(text=True)
                visible_texts = filter(self.tag_visible, texts)
                text = &#34; &#34;.join(t.strip() for t in visible_texts)
                text = text.lstrip().rstrip()
                text = text.split(&#34;,&#34;)
                clean_text = &#34;&#34;
                for sen in text:
                    if sen:
                        sen = sen.rstrip().lstrip()
                        clean_text += sen + &#34;,&#34;
                complete_text = clean_text
            else:
                # extract text content from html5
                html5 = 1
                texts = article_html5.findAll(text=True)
                visible_texts = filter(self.tag_visible, texts)
                text = &#34; &#34;.join(t.strip() for t in visible_texts)
                text = text.lstrip().rstrip()
                text = text.split(&#34;,&#34;)
                clean_text = &#34;&#34;
                for sen in text:
                    if sen:
                        sen = sen.rstrip().lstrip()
                        clean_text += sen + &#34;,&#34;
                complete_text = clean_text

            # get meta description
            description = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;description&#34;})
            if description is None:
                description = &#34;-&#34;
            else:
                description = description.get(&#34;content&#34;)

            # get meta keywords
            keywords = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;keywords&#34;})
            if keywords is None:
                keywords = &#34;-&#34;
            else:
                keywords = keywords.get(&#34;content&#34;)

            # check hot_url
            hot_link = 0
            if (self.util.count_keyword_in_text(complete_text, self.keyword) &gt;= 10) or (
                self.util.count_keyword_in_text(title, self.keyword) &gt;= 1
            ):
                hot_link = 1

            # check if the page information already exist
            if not self.db.check_value_in_table(db_connection, &#34;page_information&#34;, &#34;url&#34;, url):
                self.page_content.insert_page_information(
                    db_connection,
                    url,
                    self.crawl_id,
                    html5,
                    title,
                    description,
                    keywords,
                    complete_text,
                    hot_link,
                    &#34;MSB crawling&#34;,
                )
            else:
                self.db.close_connection(db_connection)
                return

            # extract style
            for style in soup.findAll(&#34;style&#34;):
                self.page_content.insert_page_style(db_connection, url, style)

            # extract script
            for script in soup.findAll(&#34;script&#34;):
                self.page_content.insert_page_script(db_connection, url, script)

            # extract lists
            for lists in soup.findAll(&#34;li&#34;):
                self.page_content.insert_page_list(db_connection, url, lists)

            # extract forms
            for form in soup.findAll(&#34;form&#34;):
                self.page_content.insert_page_form(db_connection, url, form)

            # extract tables
            for table in soup.findAll(&#34;table&#34;):
                self.page_content.insert_page_table(db_connection, url, table)

            # extract images
            for image in soup.findAll(&#34;img&#34;):
                self.page_content.insert_page_image(db_connection, url, image)

            # extract outgoing link
            links = soup.findAll(&#34;a&#34;, href=True)
            for i in links:
                # Complete relative URLs and strip trailing slash
                complete_url = urljoin(url, i[&#34;href&#34;]).rstrip(&#34;/&#34;)

                self.list_urls.append(complete_url)
                self.page_content.insert_page_linking(db_connection, self.crawl_id, url, complete_url)

                self.lock.acquire()
                if self.util.is_valid_url(complete_url) and complete_url not in self.visited_urls:
                    if hot_link == 1 or self.keyword in url:
                        self.hot_queue.put(complete_url)
                    else:
                        self.url_queue.put(complete_url)
                self.lock.release()

            self.db.close_connection(db_connection)
            return
        return
    except Exception as e:
        print(e, &#34;~ Error in thread&#34;)
        return</code></pre>
</details>
</dd>
<dt id="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.tag_visible"><code class="name flex">
<span>def <span class="ident">tag_visible</span></span>(<span>self, element)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi untuk merapihkan konten teks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tag_visible(self, element):
    &#34;&#34;&#34;Fungsi untuk merapihkan konten teks.&#34;&#34;&#34;
    if element.parent.name in [&#34;style&#34;, &#34;script&#34;, &#34;head&#34;, &#34;title&#34;, &#34;meta&#34;, &#34;[document]&#34;]:
        return False
    if isinstance(element, bs4.element.Comment):
        return False
    if re.match(r&#34;[\n]+&#34;, str(element)):
        return False
    return True</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="search-engine.src.crawling" href="index.html">search-engine.src.crawling</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased" href="#search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased">ModifiedSimilarityBased</a></code></h4>
<ul class="">
<li><code><a title="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.reorder_queue" href="#search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.reorder_queue">reorder_queue</a></code></li>
<li><code><a title="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.run" href="#search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.run">run</a></code></li>
<li><code><a title="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.scrape_page" href="#search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.scrape_page">scrape_page</a></code></li>
<li><code><a title="search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.tag_visible" href="#search-engine.src.crawling.modified_similarity_based.ModifiedSimilarityBased.tag_visible">tag_visible</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>